{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train, val, test sets for multimodal pipeline\n",
    "### Split: train, val, test\n",
    "### Disease: CAD, Infarction\n",
    "### Store in UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final\n",
    "\n",
    "- cardiac_features_{split}_imputed_noOH_tabular_imaging_reordered.csv\n",
    "- cardiac_{split}_paths_imaging.pt\n",
    "- labels_{disease}_{split}.pt\n",
    "- tabular_lengths_reordered.pt\n",
    "\n",
    "- cardiac_features_{split}_imputed_noOH_tabular_imaging_reordered_balanced.csv\n",
    "- cardiac_{split}_paths_imaging_balanced.pt\n",
    "- labels_{disease}_{split}_balanced.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from Utils import check_or_save\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_columns = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUTE = True\n",
    "impute_strategy = 'iterative'\n",
    "BALANCED = True\n",
    "one_hot_encoded = False\n",
    "\n",
    "# BASE = '/vol/biomedic3/sd1523/data/mm/UKBB'\n",
    "BASE = '/bigdata/siyi/data/UKBB'\n",
    "\n",
    "RAW_DATA = '/vol/biodata/data/biobank/18545/data'\n",
    "RAW_FEATURES = join(BASE, 'features')\n",
    "FEATURES = join(BASE, 'cardiac_segmentations', 'projects','SelfSuperBio', '18545')\n",
    "STORE_PATH = join(FEATURES, 'final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete ids that don't have img folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(join(RAW_FEATURES, 'cardiac_feature_18545_vector_labeled_noOH.csv'))\n",
    "categorical_columns = data_df.loc[:,'Sex-0.0':].columns\n",
    "for x in categorical_columns:\n",
    "    print(data_df[x].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate invalid_ids list\n",
    "_ids = list(data_df['eid'].astype(int))\n",
    "invalid_ids_path = join(FEATURES, 'invalid_ids.pt')\n",
    "if os.path.exists(invalid_ids_path):\n",
    "    invalid_ids = torch.load(invalid_ids_path)\n",
    "else:\n",
    "    invalid_ids = []\n",
    "    for id in tqdm(_ids):\n",
    "        path = join(RAW_DATA, str(id))\n",
    "        if not os.path.isdir(path):\n",
    "            invalid_ids.append(id)\n",
    "    print(f'Found {len(invalid_ids)} bad indices in the whole dataset')\n",
    "    check_or_save(invalid_ids, join(FEATURES, 'invalid_ids.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length before 442554\n",
      "Val length after 36194\n"
     ]
    }
   ],
   "source": [
    "# remove invalid ids\n",
    "to_del = torch.load(join(FEATURES,'invalid_ids.pt'))\n",
    "print(f'Dataset length before {len(_ids)}')\n",
    "for _id in to_del:\n",
    "    _ids.remove(_id)\n",
    "print(f'Val length after {len(_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store removed invalid ids df\n",
    "to_del = set([int(x) for x in to_del])\n",
    "new_data_df = data_df[~data_df['eid'].isin(to_del)]\n",
    "new_data_df.reset_index(inplace=True, drop=True)\n",
    "new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_or_save(new_data_df, join(FEATURES, 'cardiac_feature_18545_vector_labeled_noOH_dropNI.csv'), index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute and split tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36194, 81)\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(join(FEATURES, 'cardiac_feature_18545_vector_labeled_noOH_dropNI.csv'))\n",
    "_ids = list(data_df['eid'].astype(int))\n",
    "print(data_df.shape)\n",
    "all_data_df = data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_coverage = data_df.notna().sum()/len(data_df)*100\n",
    "data_df_coverage.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of problem ids in cardiac image: 27\n",
      "Dataset length before 36194\n",
      "Val length after 36167\n"
     ]
    }
   ],
   "source": [
    "# If no train, val, test sets\n",
    "problem_ids = torch.load(join(FEATURES, 'problem_ids_cardiac.pt'))\n",
    "print(f'Num of problem ids in cardiac image: {len(problem_ids)}')\n",
    "print(f'Dataset length before {len(_ids)}')\n",
    "for _id in problem_ids:\n",
    "    _ids.remove(_id)\n",
    "print(f'Val length after {len(_ids)}')\n",
    "\n",
    "train_set_ids, test_ids = train_test_split(_ids, test_size=0.1, random_state=2022)\n",
    "train_ids, val_ids = train_test_split(train_set_ids, test_size=0.2, random_state=2022)\n",
    "\n",
    "check_or_save(train_ids, join(FEATURES,'ids_train_tabular_imaging.pt'))\n",
    "check_or_save(val_ids, join(FEATURES,'ids_val_tabular_imaging.pt'))\n",
    "check_or_save(test_ids, join(FEATURES,'ids_test_tabular_imaging.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 26040, val: 6510, test: 3617, total: 36167\n"
     ]
    }
   ],
   "source": [
    "train_ids = torch.load(join(FEATURES, 'ids_train_tabular_imaging.pt'))\n",
    "val_ids = torch.load(join(FEATURES, 'ids_val_tabular_imaging.pt'))\n",
    "test_ids = torch.load(join(FEATURES, 'ids_test_tabular_imaging.pt'))\n",
    "print(f'train: {len(train_ids)}, val: {len(val_ids)}, test: {len(test_ids)}, total: {len(train_ids)+len(val_ids)+len(test_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_df = data_df.loc[:,'eid'].astype('int')\n",
    "# eid_df = data_df.loc[:,'eid_old'].astype('int')\n",
    "\n",
    "continuous_df = data_df.loc[:,(\n",
    "    'Pulse wave Arterial Stiffness index-2.0',\n",
    "    'Systolic blood pressure-2.mean',\n",
    "    'Diastolic blood pressure-2.mean',\n",
    "    'Pulse rate-2.mean',\n",
    "    'Body fat percentage-2.0',\n",
    "    'Whole body fat mass-2.0',\n",
    "    # 'Whole body fat-free mass-2.0',\n",
    "    # 'Whole body water mass-2.0',\n",
    "    'Body mass index (BMI)-2.0',\n",
    "    # 'Cooked vegetable intake-2.0',\n",
    "    # 'Salad / raw vegetable intake-2.0',\n",
    "    # 'Cardiac operations performed',\n",
    "    # 'Total mass-2.0',\n",
    "    'Basal metabolic rate-2.0',\n",
    "    # 'Impedance of whole body-2.0',\n",
    "    'Waist circumference-2.0',\n",
    "    'Hip circumference-2.0',\n",
    "    # 'Standing height-2.0',\n",
    "    # 'Height-2.0',\n",
    "    # 'Sitting height-2.0',\n",
    "    'Weight-2.0',\n",
    "    'Ventricular rate-2.0',\n",
    "    'P duration-2.0',\n",
    "    'QRS duration-2.0',\n",
    "    # 'PQ interval-2.0',\n",
    "    # 'RR interval-2.0',\n",
    "    # 'PP interval-2.0',\n",
    "    'Cardiac output-2.0',\n",
    "    'Cardiac index-2.0',\n",
    "    'Average heart rate-2.0',\n",
    "    'Body surface area-2.0',\n",
    "    'Duration of walks-2.0',\n",
    "    'Duration of moderate activity-2.0',\n",
    "    'Duration of vigorous activity-2.0',\n",
    "    # 'Time spent watching television (TV)-2.0',\n",
    "    # 'Time spent using computer-2.0',\n",
    "    # 'Time spent driving-2.0',\n",
    "    'Heart rate during PWA-2.0',\n",
    "    'Systolic brachial blood pressure during PWA-2.0',\n",
    "    'Diastolic brachial blood pressure during PWA-2.0',\n",
    "    'Peripheral pulse pressure during PWA-2.0',\n",
    "    'Central systolic blood pressure during PWA-2.0',\n",
    "    'Central pulse pressure during PWA-2.0',\n",
    "    'Number of beats in waveform average for PWA-2.0',\n",
    "    'Central augmentation pressure during PWA-2.0',\n",
    "    'Augmentation index for PWA-2.0',\n",
    "    'Cardiac output during PWA-2.0',\n",
    "    'End systolic pressure during PWA-2.0',\n",
    "    'End systolic pressure index during PWA-2.0',\n",
    "    'Total peripheral resistance during PWA-2.0',\n",
    "    'Stroke volume during PWA-2.0',\n",
    "    # 'Mean arterial pressure during PWA-2.0',\n",
    "    'Cardiac index during PWA-2.0',\n",
    "    'Sleep duration-2.0',\n",
    "    'Exposure to tobacco smoke at home-2.0',\n",
    "    'Exposure to tobacco smoke outside home-2.0',\n",
    "    # 'Pack years of smoking-2.0',\n",
    "    # 'Pack years adult smoking as proportion of life span exposed to smoking-2.0',\n",
    "    'LVESV (mL)',\n",
    "    'LVEDV (mL)',\n",
    "    'LVSV (mL)',\n",
    "    'LVEF (%)',\n",
    "    'LVCO (L/min)',\n",
    "    'LVM (g)',\n",
    "    'RVEDV (mL)',\n",
    "    'RVESV (mL)', \n",
    "    'RVSV (mL)', \n",
    "    'RVEF (%)',\n",
    ")]\n",
    "if one_hot_encoded:\n",
    "  categorical_df = data_df.loc[:,(\n",
    "    'Worrier / anxious feelings-2.0',\n",
    "    'Shortness of breath walking on level ground-2.0',\n",
    "    'Sex-0.0',\n",
    "    'Diabetes diagnosis',\n",
    "    #'Heart attack diagnosed by doctor',   # commented by LaaF\n",
    "    'Angina diagnosed by doctor',\n",
    "    'Stroke diagnosed by doctor',\n",
    "    'High blood pressure diagnosed by doctor',\n",
    "    'Cholesterol lowering medication regularly taken',\n",
    "    'Blood pressure medication regularly taken',\n",
    "    'Insulin medication regularly taken',\n",
    "    'Hormone replacement therapy medication regularly taken',\n",
    "    'Oral contraceptive pill or minipill medication regularly taken',\n",
    "    'Pace-maker-2.0',\n",
    "    'Ever had diabetes (Type I or Type II)-0.0',\n",
    "    'Long-standing illness, disability or infirmity-2.0',\n",
    "    'Tense / \\'highly strung\\'-2.0',\n",
    "    'Ever smoked-2.0',\n",
    "\n",
    "    'Alcohol intake frequency.-2.0-0',\n",
    "    'Alcohol intake frequency.-2.0-1',\n",
    "    'Alcohol intake frequency.-2.0-2',\n",
    "    'Alcohol intake frequency.-2.0-3',\n",
    "    'Alcohol intake frequency.-2.0-4',\n",
    "    'Alcohol intake frequency.-2.0-5',\n",
    "    'Processed meat intake-2.0-0',\n",
    "    'Processed meat intake-2.0-1',\n",
    "    'Processed meat intake-2.0-2',\n",
    "    'Processed meat intake-2.0-3',\n",
    "    'Processed meat intake-2.0-4',\n",
    "    'Processed meat intake-2.0-5',\n",
    "    'Beef intake-2.0-0',\n",
    "    'Beef intake-2.0-1',\n",
    "    'Beef intake-2.0-2',\n",
    "    'Beef intake-2.0-3',\n",
    "    'Beef intake-2.0-4',\n",
    "    'Beef intake-2.0-5',\n",
    "    'Pork intake-2.0-0',\n",
    "    'Pork intake-2.0-1',\n",
    "    'Pork intake-2.0-2',\n",
    "    'Pork intake-2.0-3',\n",
    "    'Pork intake-2.0-4',\n",
    "    'Pork intake-2.0-5',\n",
    "    'Lamb/mutton intake-2.0-0',\n",
    "    'Lamb/mutton intake-2.0-1',\n",
    "    'Lamb/mutton intake-2.0-2',\n",
    "    'Lamb/mutton intake-2.0-3',\n",
    "    'Lamb/mutton intake-2.0-4',\n",
    "    'Lamb/mutton intake-2.0-5',\n",
    "    'Overall health rating-2.0-0',\n",
    "    'Overall health rating-2.0-1',\n",
    "    'Overall health rating-2.0-2',\n",
    "    'Overall health rating-2.0-3',\n",
    "    'Alcohol usually taken with meals-2.0-0',\n",
    "    'Alcohol usually taken with meals-2.0-1',\n",
    "    'Alcohol usually taken with meals-2.0-2',\n",
    "    'Alcohol drinker status-2.0-0',\n",
    "    'Alcohol drinker status-2.0-1',\n",
    "    'Alcohol drinker status-2.0-2',\n",
    "    'Frequency of drinking alcohol-0.0-0',\n",
    "    'Frequency of drinking alcohol-0.0-1',\n",
    "    'Frequency of drinking alcohol-0.0-2',\n",
    "    'Frequency of drinking alcohol-0.0-3',\n",
    "    'Frequency of drinking alcohol-0.0-4',\n",
    "    'Frequency of consuming six or more units of alcohol-0.0-0',\n",
    "    'Frequency of consuming six or more units of alcohol-0.0-1',\n",
    "    'Frequency of consuming six or more units of alcohol-0.0-2',\n",
    "    'Frequency of consuming six or more units of alcohol-0.0-3',\n",
    "    'Frequency of consuming six or more units of alcohol-0.0-4',\n",
    "    'Amount of alcohol drunk on a typical drinking day-0.0-0',\n",
    "    'Amount of alcohol drunk on a typical drinking day-0.0-1',\n",
    "    'Amount of alcohol drunk on a typical drinking day-0.0-2',\n",
    "    'Amount of alcohol drunk on a typical drinking day-0.0-3',\n",
    "    'Amount of alcohol drunk on a typical drinking day-0.0-4',\n",
    "    'Amount of alcohol drunk on a typical drinking day-0.0-5',\n",
    "    'Falls in the last year-2.0-0',\n",
    "    'Falls in the last year-2.0-1',\n",
    "    'Falls in the last year-2.0-2',\n",
    "    'Weight change compared with 1 year ago-2.0-0',\n",
    "    'Weight change compared with 1 year ago-2.0-1',\n",
    "    'Weight change compared with 1 year ago-2.0-2',\n",
    "    'Number of days/week walked 10+ minutes-2.0-0',\n",
    "    'Number of days/week walked 10+ minutes-2.0-1',\n",
    "    'Number of days/week walked 10+ minutes-2.0-2',\n",
    "    'Number of days/week walked 10+ minutes-2.0-3',\n",
    "    'Number of days/week walked 10+ minutes-2.0-4',\n",
    "    'Number of days/week walked 10+ minutes-2.0-5',\n",
    "    'Number of days/week walked 10+ minutes-2.0-6',\n",
    "    'Number of days/week walked 10+ minutes-2.0-7',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-0',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-1',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-2',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-3',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-4',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-5',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-6',\n",
    "    'Number of days/week of moderate physical activity 10+ minutes-2.0-7',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-0',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-1',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-2',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-3',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-4',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-5',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-6',\n",
    "    'Number of days/week of vigorous physical activity 10+ minutes-2.0-7',\n",
    "    'Usual walking pace-2.0-0',\n",
    "    'Usual walking pace-2.0-1',\n",
    "    'Usual walking pace-2.0-2',\n",
    "    'Frequency of stair climbing in last 4 weeks-2.0-0',\n",
    "    'Frequency of stair climbing in last 4 weeks-2.0-1',\n",
    "    'Frequency of stair climbing in last 4 weeks-2.0-2',\n",
    "    'Frequency of stair climbing in last 4 weeks-2.0-3',\n",
    "    'Frequency of stair climbing in last 4 weeks-2.0-4',\n",
    "    'Frequency of stair climbing in last 4 weeks-2.0-5',\n",
    "    'Frequency of walking for pleasure in last 4 weeks-2.0-0',\n",
    "    'Frequency of walking for pleasure in last 4 weeks-2.0-1',\n",
    "    'Frequency of walking for pleasure in last 4 weeks-2.0-2',\n",
    "    'Frequency of walking for pleasure in last 4 weeks-2.0-3',\n",
    "    'Frequency of walking for pleasure in last 4 weeks-2.0-4',\n",
    "    'Frequency of walking for pleasure in last 4 weeks-2.0-5',\n",
    "    'Frequency of walking for pleasure in last 4 weeks-2.0-6',\n",
    "    'Duration walking for pleasure-2.0-0',\n",
    "    'Duration walking for pleasure-2.0-1',\n",
    "    'Duration walking for pleasure-2.0-2',\n",
    "    'Duration walking for pleasure-2.0-3',\n",
    "    'Duration walking for pleasure-2.0-4',\n",
    "    'Duration walking for pleasure-2.0-5',\n",
    "    'Duration walking for pleasure-2.0-6',\n",
    "    'Duration walking for pleasure-2.0-7',\n",
    "    'Frequency of strenuous sports in last 4 weeks-2.0-0',\n",
    "    'Frequency of strenuous sports in last 4 weeks-2.0-1',\n",
    "    'Frequency of strenuous sports in last 4 weeks-2.0-2',\n",
    "    'Frequency of strenuous sports in last 4 weeks-2.0-3',\n",
    "    'Frequency of strenuous sports in last 4 weeks-2.0-4',\n",
    "    'Frequency of strenuous sports in last 4 weeks-2.0-5',\n",
    "    'Frequency of strenuous sports in last 4 weeks-2.0-6',\n",
    "    'Duration of strenuous sports-2.0-0',\n",
    "    'Duration of strenuous sports-2.0-1',\n",
    "    'Duration of strenuous sports-2.0-2',\n",
    "    'Duration of strenuous sports-2.0-3',\n",
    "    'Duration of strenuous sports-2.0-4',\n",
    "    'Duration of strenuous sports-2.0-5',\n",
    "    'Duration of strenuous sports-2.0-6',\n",
    "    'Duration of strenuous sports-2.0-7',\n",
    "    'Duration of light DIY-2.0-0',\n",
    "    'Duration of light DIY-2.0-1',\n",
    "    'Duration of light DIY-2.0-2',\n",
    "    'Duration of light DIY-2.0-3',\n",
    "    'Duration of light DIY-2.0-4',\n",
    "    'Duration of light DIY-2.0-5',\n",
    "    'Duration of light DIY-2.0-6',\n",
    "    'Duration of light DIY-2.0-7',\n",
    "    'Frequency of heavy DIY in last 4 weeks-2.0-0',\n",
    "    'Frequency of heavy DIY in last 4 weeks-2.0-1',\n",
    "    'Frequency of heavy DIY in last 4 weeks-2.0-2',\n",
    "    'Frequency of heavy DIY in last 4 weeks-2.0-3',\n",
    "    'Frequency of heavy DIY in last 4 weeks-2.0-4',\n",
    "    'Frequency of heavy DIY in last 4 weeks-2.0-5',\n",
    "    'Frequency of heavy DIY in last 4 weeks-2.0-6',\n",
    "    'Duration of heavy DIY-2.0-0',\n",
    "    'Duration of heavy DIY-2.0-1',\n",
    "    'Duration of heavy DIY-2.0-2',\n",
    "    'Duration of heavy DIY-2.0-3',\n",
    "    'Duration of heavy DIY-2.0-4',\n",
    "    'Duration of heavy DIY-2.0-5',\n",
    "    'Duration of heavy DIY-2.0-6',\n",
    "    'Duration of heavy DIY-2.0-7',\n",
    "    'Frequency of other exercises in last 4 weeks-2.0-0',\n",
    "    'Frequency of other exercises in last 4 weeks-2.0-1',\n",
    "    'Frequency of other exercises in last 4 weeks-2.0-2',\n",
    "    'Frequency of other exercises in last 4 weeks-2.0-3',\n",
    "    'Frequency of other exercises in last 4 weeks-2.0-4',\n",
    "    'Frequency of other exercises in last 4 weeks-2.0-5',\n",
    "    'Frequency of other exercises in last 4 weeks-2.0-6',\n",
    "    'Duration of other exercises-2.0-0',\n",
    "    'Duration of other exercises-2.0-1',\n",
    "    'Duration of other exercises-2.0-2',\n",
    "    'Duration of other exercises-2.0-3',\n",
    "    'Duration of other exercises-2.0-4',\n",
    "    'Duration of other exercises-2.0-5',\n",
    "    'Duration of other exercises-2.0-6',\n",
    "    'Duration of other exercises-2.0-7',\n",
    "    'Sleeplessness / insomnia-2.0-0',\n",
    "    'Sleeplessness / insomnia-2.0-1',\n",
    "    'Sleeplessness / insomnia-2.0-2',\n",
    "    'Current tobacco smoking-2.0-0',\n",
    "    'Current tobacco smoking-2.0-1',\n",
    "    'Current tobacco smoking-2.0-2',\n",
    "    'Past tobacco smoking-2.0-0',\n",
    "    'Past tobacco smoking-2.0-1',\n",
    "    'Past tobacco smoking-2.0-2',\n",
    "    'Past tobacco smoking-2.0-3',\n",
    "    'Smoking/smokers in household-2.0-0',\n",
    "    'Smoking/smokers in household-2.0-1',\n",
    "    'Smoking/smokers in household-2.0-2',\n",
    "    'Smoking status-2.0-0',\n",
    "    'Smoking status-2.0-1',\n",
    "    'Smoking status-2.0-2'\n",
    ")]\n",
    "else:\n",
    "  categorical_df = data_df.loc[:,(\n",
    "    # 'Worrier / anxious feelings-2.0',\n",
    "    'Shortness of breath walking on level ground-2.0',\n",
    "    'Sex-0.0',\n",
    "    'Diabetes diagnosis',\n",
    "    #'Heart attack diagnosed by doctor',  # commented by LaaF\n",
    "    'Angina diagnosed by doctor',\n",
    "    'Stroke diagnosed by doctor',\n",
    "    'High blood pressure diagnosed by doctor',\n",
    "    'Cholesterol lowering medication regularly taken',\n",
    "    'Blood pressure medication regularly taken',\n",
    "    'Insulin medication regularly taken',\n",
    "    'Hormone replacement therapy medication regularly taken',\n",
    "    'Oral contraceptive pill or minipill medication regularly taken',\n",
    "    # 'Pace-maker-2.0',\n",
    "    # 'Ever had diabetes (Type I or Type II)-0.0',\n",
    "    'Long-standing illness, disability or infirmity-2.0',\n",
    "    # 'Tense / \\'highly strung\\'-2.0',\n",
    "    'Ever smoked-2.0',\n",
    "\n",
    "    \"Sleeplessness / insomnia-2.0\",\n",
    "    # \"Frequency of heavy DIY in last 4 weeks-2.0\",\n",
    "    \"Alcohol intake frequency.-2.0\",\n",
    "    # \"Processed meat intake-2.0\",\n",
    "    # \"Beef intake-2.0\",\n",
    "    # \"Pork intake-2.0\",\n",
    "    # \"Lamb/mutton intake-2.0\",\n",
    "    \"Overall health rating-2.0\",\n",
    "    # \"Alcohol usually taken with meals-2.0\",\n",
    "    \"Alcohol drinker status-2.0\",\n",
    "    # \"Frequency of drinking alcohol-0.0\",\n",
    "    # \"Frequency of consuming six or more units of alcohol-0.0\",\n",
    "    # \"Amount of alcohol drunk on a typical drinking day-0.0\",\n",
    "    \"Falls in the last year-2.0\",\n",
    "    # \"Weight change compared with 1 year ago-2.0\",\n",
    "    \"Number of days/week walked 10+ minutes-2.0\",\n",
    "    \"Number of days/week of moderate physical activity 10+ minutes-2.0\",\n",
    "    \"Number of days/week of vigorous physical activity 10+ minutes-2.0\",\n",
    "    \"Usual walking pace-2.0\",\n",
    "    # \"Frequency of stair climbing in last 4 weeks-2.0\",\n",
    "    # \"Frequency of walking for pleasure in last 4 weeks-2.0\",\n",
    "    # \"Duration walking for pleasure-2.0\",\n",
    "    # \"Frequency of strenuous sports in last 4 weeks-2.0\",\n",
    "    \"Duration of strenuous sports-2.0\",\n",
    "    # \"Duration of light DIY-2.0\",\n",
    "    # \"Duration of heavy DIY-2.0\",\n",
    "    # \"Frequency of other exercises in last 4 weeks-2.0\",\n",
    "    # \"Duration of other exercises-2.0\",\n",
    "    \"Current tobacco smoking-2.0\",\n",
    "    \"Past tobacco smoking-2.0\",\n",
    "    # \"Smoking/smokers in household-2.0\",\n",
    "    \"Smoking status-2.0\"\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of continuous features: 49\n",
      "Number of samples: 36194\n",
      "Number of categorical features: 26\n",
      "data_df shape: (36194, 81)\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "continuous_df=(continuous_df-continuous_df.mean())/continuous_df.std()\n",
    "print(f'Number of continuous features: {len(continuous_df.columns)}')\n",
    "print(f'Number of samples: {len(continuous_df)}')\n",
    "print(f'Number of categorical features: {len(categorical_df.columns)}')\n",
    "print(f'data_df shape: {data_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2489814/2206879864.py:32: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data_df.loc[:,categorical_df.columns] = data_df_nn_imputed[:,categorical_column_indices]\n"
     ]
    }
   ],
   "source": [
    "if IMPUTE:\n",
    "  if impute_strategy.lower()=='simple':\n",
    "    continuous_df.fillna(0, inplace=True)\n",
    "\n",
    "    for i in categorical_df.columns[categorical_df.isnull().any(axis=0)]:\n",
    "        categorical_df[i].fillna(categorical_df[i].mode()[0], inplace=True)\n",
    "\n",
    "    data_df = pd.concat([eid_df, continuous_df, categorical_df], axis=1)\n",
    "  else:\n",
    "    data_df = pd.concat([eid_df, continuous_df, categorical_df], axis=1)\n",
    "\n",
    "    mi = 10\n",
    "    nn = 9\n",
    "    it_path = os.path.join(FEATURES, f'data_df_imputed_{mi}.pt')\n",
    "    knn_path = os.path.join(FEATURES, f'data_df_NNimputed_{nn}.pt')\n",
    "    if os.path.exists(it_path):\n",
    "      continuous_df_it_imputed = torch.load(it_path)\n",
    "    else:\n",
    "      imp = IterativeImputer(max_iter=mi, random_state=0, sample_posterior=True, skip_complete=True, min_value=continuous_df.min(), max_value=continuous_df.max())\n",
    "      continuous_df_it_imputed = imp.fit_transform(continuous_df)\n",
    "      torch.save(continuous_df_it_imputed, os.path.join(FEATURES, f'data_df_imputed_{mi}.pt'))\n",
    "\n",
    "    if os.path.exists(knn_path):\n",
    "      data_df_nn_imputed = torch.load(knn_path)\n",
    "    else:\n",
    "      knnimputer = KNNImputer(n_neighbors=nn)\n",
    "      data_df_nn_imputed = knnimputer.fit_transform(data_df.iloc[:,1:]) # exclude eid\n",
    "      torch.save(data_df_nn_imputed, os.path.join(FEATURES, f'data_df_NNimputed_{nn}.pt'))\n",
    "\n",
    "    categorical_column_indices = [data_df.columns.get_loc(c) for c in data_df.columns if c in categorical_df.columns]\n",
    "    categorical_column_indices = [c-1 for c in categorical_column_indices] # shift by one because no more eid\n",
    "    data_df.loc[:,categorical_df.columns] = data_df_nn_imputed[:,categorical_column_indices]\n",
    "    data_df.loc[:,categorical_df.columns] = data_df.loc[:,categorical_df.columns].round(0) # round to nearest integer because categorical can only be integer\n",
    "\n",
    "    data_df.loc[:,continuous_df.columns] = continuous_df_it_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36194, 76)\n"
     ]
    }
   ],
   "source": [
    "print(data_df.shape)\n",
    "check_or_save(data_df, join(FEATURES, 'cardiac_feature_18545_vector_labeled_noOH_dropNI_imputed.csv'), index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 6, 4, 3, 3, 8, 8, 8, 3, 8, 3, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "lengths = [1 for i in range(len(continuous_df.columns))]\n",
    "max = list(data_df.max(axis=0))[len(continuous_df.columns)+1:]\n",
    "max = [int(i)+1 for i in max]\n",
    "lengths = lengths + max\n",
    "\n",
    "check_or_save(lengths, join(FEATURES, 'tabular_lengths.pt'))\n",
    "print(len(lengths), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Index: 26,  [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n",
      "Numerical Index: 49,  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n"
     ]
    }
   ],
   "source": [
    "field_lengths_tabular = lengths\n",
    "categorical_ids = []\n",
    "continuous_ids = []\n",
    "for i in range(len(field_lengths_tabular)):\n",
    "    if field_lengths_tabular[i] == 1:\n",
    "        continuous_ids.append(i)\n",
    "    else:\n",
    "        categorical_ids.append(i)\n",
    "print('Categorical Index: {}, '.format(len(categorical_ids)), categorical_ids)\n",
    "print('Numerical Index: {}, '.format(len(continuous_ids)), continuous_ids)\n",
    "\n",
    "reorder_ids = categorical_ids + continuous_ids\n",
    "reorder_field_lengths_tabular = [field_lengths_tabular[i] for i in reorder_ids]\n",
    "reorder_field_lengths_tabular_noExt = reorder_field_lengths_tabular[:-10]\n",
    "check_or_save(reorder_field_lengths_tabular, join(STORE_PATH, f'tabular_lengths_reordered.pt'), index=False, header=False)\n",
    "check_or_save(reorder_field_lengths_tabular_noExt, join(STORE_PATH, f'tabular_lengths_reordered_noExt.pt'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train, val, test tabular features, labels, and image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36167\n"
     ]
    }
   ],
   "source": [
    "all_image_paths = torch.load(join(FEATURES, 'preprocessed_cardiac_npy_path.pt'))\n",
    "print(len(all_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infarction\n",
      "train df shape: (26040, 75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train df shape: (26040, 65)\n",
      "train image path shape: 26040\n",
      "train label shape: 26040\n",
      "val df shape: (6510, 75)\n",
      "val df shape: (6510, 65)\n",
      "val image path shape: 6510\n",
      "val label shape: 6510\n",
      "test df shape: (3617, 75)\n",
      "test df shape: (3617, 65)\n",
      "test image path shape: 3617\n",
      "test label shape: 3617\n"
     ]
    }
   ],
   "source": [
    "target = 'Infarction'\n",
    "# target = 'CAD'\n",
    "# directly save\n",
    "print(target)\n",
    "for split, ids in zip(['train', 'val', 'test'], [train_ids, val_ids, test_ids]):\n",
    "    # tabular features\n",
    "    split_df = data_df.set_index('eid').loc[ids]\n",
    "    split_df = split_df.iloc[:,reorder_ids]\n",
    "    print(f'{split} df shape: {split_df.shape}')\n",
    "    check_or_save(split_df, join(STORE_PATH, f'cardiac_features_{split}_imputed_noOH_tabular_imaging_reordered.csv'), index=False, header=False)\n",
    "    # tabular features no extracted\n",
    "    split_df_no_extracted = split_df.iloc[:,:-10]\n",
    "    print(f'{split} df shape: {split_df_no_extracted.shape}')\n",
    "    check_or_save(split_df_no_extracted, join(STORE_PATH, f'cardiac_features_{split}_imputed_noOH_tabular_imaging_reordered_noExt.csv'), index=False, header=False)\n",
    "    # image paths\n",
    "    split_image_paths = [all_image_paths[k] for k in ids]\n",
    "    check_or_save(split_image_paths, join(STORE_PATH, f'cardiac_{split}_paths_imaging.pt'))\n",
    "    print(f'{split} image path shape: {len(split_image_paths)}')\n",
    "    # labels\n",
    "    split_all_df = all_data_df.set_index('eid').loc[ids]\n",
    "    split_labels = split_all_df[target].values \n",
    "    check_or_save(split_labels, join(STORE_PATH, f'cardiac_labels_{target}_{split}.pt'))\n",
    "    print(f'{split} label shape: {len(split_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infarction\n",
      "train tabular feature shape: (1552, 75)\n",
      "train tabular feature shape: (1552, 65)\n",
      "train image path shape: 1552\n",
      "train label shape: 1552\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/ids_train_tabular_imaging_Infarction_balanced.pt\n",
      "val tabular feature shape: (472, 75)\n",
      "val tabular feature shape: (472, 65)\n",
      "val image path shape: 472\n",
      "val label shape: 472\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/ids_val_tabular_imaging_Infarction_balanced.pt\n"
     ]
    }
   ],
   "source": [
    "# balance\n",
    "# data_df contains final imputed features, all_data_df contains all features and labels\n",
    "target = 'Infarction'\n",
    "# target = 'CAD'\n",
    "data_df['eid_old'] = data_df['eid']\n",
    "all_data_df['eid_old'] = all_data_df['eid']\n",
    "train_df = data_df.set_index('eid_old').loc[train_ids]\n",
    "val_df = data_df.set_index('eid_old').loc[val_ids]\n",
    "all_train_df = all_data_df.set_index('eid_old').loc[train_ids]\n",
    "all_val_df = all_data_df.set_index('eid_old').loc[val_ids]\n",
    "print(target)\n",
    "\n",
    "for all_split_df, split_df, split in zip([all_train_df, all_val_df], [train_df, val_df], ['train', 'val']):\n",
    "    pos_eids = list(all_split_df.loc[all_split_df[target] == 1]['eid'])\n",
    "    random.seed(2022)\n",
    "    neg_eids = random.sample(list(all_split_df.loc[all_split_df[target] == 0]['eid']), len(pos_eids))\n",
    "    all_eids = pos_eids + neg_eids\n",
    "    # tabular features\n",
    "    balanced_split_df = split_df.set_index('eid').loc[all_eids]\n",
    "    balanced_split_df = balanced_split_df.iloc[:,reorder_ids]\n",
    "    print(f'{split} tabular feature shape: {balanced_split_df.shape}')\n",
    "    check_or_save(balanced_split_df, join(STORE_PATH, f'cardiac_features_{split}_imputed_noOH_tabular_imaging_{target}_balanced_reordered.csv'), index=False, header=False)\n",
    "    # tabular features no extracted\n",
    "    balanced_split_df_no_extracted = balanced_split_df.iloc[:,:-10]\n",
    "    print(f'{split} tabular feature shape: {balanced_split_df_no_extracted.shape}')\n",
    "    check_or_save(balanced_split_df_no_extracted, join(STORE_PATH, f'cardiac_features_{split}_imputed_noOH_tabular_imaging_{target}_balanced_reordered_noExt.csv'), index=False, header=False)\n",
    "    # image paths\n",
    "    balanced_split_image_paths = [all_image_paths[k] for k in all_eids]\n",
    "    check_or_save(balanced_split_image_paths, join(STORE_PATH, f'cardiac_{split}_paths_imaging_{target}_balanced.pt'))\n",
    "    print(f'{split} image path shape: {len(balanced_split_image_paths)}')\n",
    "    # labels\n",
    "    split_all_df = all_data_df.set_index('eid').loc[all_eids]\n",
    "    split_labels = split_all_df[target].values\n",
    "    check_or_save(split_labels, join(STORE_PATH, f'cardiac_labels_{target}_{split}_balanced.pt'))\n",
    "    print(f'{split} label shape: {len(split_labels)}')\n",
    "    # ids\n",
    "    check_or_save(all_eids, join(STORE_PATH, f'ids_{split}_tabular_imaging_{target}_balanced.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get low balanced data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAD\n",
      "Low data 0.1 ids shape: 349\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/ids_train_tabular_imaging_CAD_0.1_balanced.pt\n",
      "Low data 0.1 labels shape: 349\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/cardiac_labels_CAD_0.1_train_balanced.pt\n",
      "Low data 0.1 image paths shape: 349\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/cardiac_train_paths_imaging_CAD_0.1_balanced.pt\n",
      "Low data 0.1 tabular features shape: (349, 75)\n",
      "Low data 0.01 ids shape: 35\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/ids_train_tabular_imaging_CAD_0.01_balanced.pt\n",
      "Low data 0.01 labels shape: 35\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/cardiac_labels_CAD_0.01_train_balanced.pt\n",
      "Low data 0.01 image paths shape: 35\n",
      "Saving to /bigdata/siyi/data/UKBB/cardiac_segmentations/projects/SelfSuperBio/18545/final/cardiac_train_paths_imaging_CAD_0.01_balanced.pt\n",
      "Low data 0.01 tabular features shape: (35, 75)\n"
     ]
    }
   ],
   "source": [
    "split='train'\n",
    "# target = 'Infarction'\n",
    "target = 'CAD'\n",
    "print(target)\n",
    "for k, prev_k in zip([0.1,0.01],['','_0.1']):\n",
    "  imputed_df = pd.read_csv(join(STORE_PATH, f'cardiac_features_{split}_imputed_noOH_tabular_imaging_{target}{prev_k}_balanced_reordered.csv'), header=None)\n",
    "  labels = torch.load(join(STORE_PATH,f'cardiac_labels_{target}{prev_k}_{split}_balanced.pt'))\n",
    "  image_paths = torch.load(join(STORE_PATH, f'cardiac_{split}_paths_imaging_{target}{prev_k}_balanced.pt'))\n",
    "  ids = torch.load(join(STORE_PATH, f'ids_{split}_tabular_imaging_{target}{prev_k}_balanced.pt'))\n",
    "  assert len(imputed_df) == len(labels) == len(image_paths) == len(ids)\n",
    "\n",
    "  indices = list(range(imputed_df.shape[0]))\n",
    "  _, low_data_indices = train_test_split(indices, test_size=0.1, random_state=2022, stratify=labels)\n",
    "  # print(low_data_indices)\n",
    "  # ids\n",
    "  low_data_ids = [ids[i] for i in low_data_indices]\n",
    "  print(f'Low data {k} ids shape: {len(low_data_ids)}')\n",
    "  check_or_save(low_data_ids, join(STORE_PATH,f'ids_{split}_tabular_imaging_{target}_{k}_balanced.pt'))\n",
    "  # labels\n",
    "  low_data_labels = [labels[i] for i in low_data_indices]\n",
    "  print(f'Low data {k} labels shape: {len(low_data_labels)}')\n",
    "  check_or_save(low_data_labels, join(STORE_PATH,f'cardiac_labels_{target}_{k}_{split}_balanced.pt'))\n",
    "  # image paths\n",
    "  low_data_image_paths = [image_paths[i] for i in low_data_indices]\n",
    "  print(f'Low data {k} image paths shape: {len(low_data_image_paths)}')\n",
    "  check_or_save(low_data_image_paths, join(STORE_PATH,f'cardiac_{split}_paths_imaging_{target}_{k}_balanced.pt'))\n",
    "  # tabular features\n",
    "  low_data_imputed_df = imputed_df.iloc[low_data_indices]\n",
    "  print(f'Low data {k} tabular features shape: {low_data_imputed_df.shape}')\n",
    "  check_or_save(low_data_imputed_df, join(STORE_PATH,f'cardiac_features_{split}_imputed_noOH_tabular_imaging_{target}_{k}_balanced_reordered.csv'), header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "target = 'Infarction'\n",
    "k = ''\n",
    "data_df = pd.read_csv(join(FEATURES, f'cardiac_feature_18545_vector_labeled_noOH_dropNI_imputed.csv'))\n",
    "all_data_df = pd.read_csv(join(FEATURES, f'cardiac_feature_18545_vector_labeled_noOH_dropNI.csv'))\n",
    "\n",
    "split_df = pd.read_csv(join(STORE_PATH, f'cardiac_features_{split}_imputed_noOH_tabular_imaging_{target}{k}_balanced_reordered.csv'), header=None)\n",
    "split_labels = torch.load(join(STORE_PATH, f'cardiac_labels_{target}{k}_{split}_balanced.pt'))\n",
    "split_image_paths = torch.load(join(STORE_PATH, f'cardiac_{split}_paths_imaging_{target}{k}_balanced.pt'))\n",
    "split_ids = torch.load(join(STORE_PATH, f'ids_{split}_tabular_imaging_{target}{k}_balanced.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552\n"
     ]
    }
   ],
   "source": [
    "print(len(split_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = [int(x.split('/')[-2]) for x in split_image_paths[:10]]\n",
    "ids = split_ids[:10]\n",
    "split_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.set_index('eid').loc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df.set_index('eid').loc[ids, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1221869, 1560136, 3442183, 1514007, 1892612, 4576079, 3905249, 5998757, 3441553, 1053366]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/2353623/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/2223143/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/2444884/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/3411878/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/5043249/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/4809059/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/2510815/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/2321244/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/2708178/sa_es_ed_mm.npy',\n",
       " '/bigdata/siyi/data/UKBB/cardiac_segmentations/subjects/5747266/sa_es_ed_mm.npy']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_image_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(split_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(len(reorder_field_lengths_tabular_noExt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2030)\n",
    "L = 10\n",
    "f, axarr = plt.subplots(L, 3, figsize=(20,5*L))\n",
    "for i in range(L):\n",
    "    rand_idx = random.randrange(0,len(split_image_paths))\n",
    "    img = np.load(split_image_paths[rand_idx])\n",
    "    assert img.shape == (210,210,3)\n",
    "    # print(f'max: {np.max(img, axis=(0,1))}, min: {np.min(img, axis=(0,1))}')\n",
    "    # print(f'mean: {np.mean(img, axis=(0,1))}')\n",
    "    axarr[i,0].imshow(img[:,:,0])\n",
    "    axarr[i,1].imshow(img[:,:,1])\n",
    "    axarr[i,2].imshow(img[:,:,2])\n",
    "print(img.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "RAW_DATA = '/vol/biodata/data/biobank/18545/data'\n",
    "not_matching_ids = torch.load(join(FEATURES, 'not_matching_ids_cardiac.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000690, 1001312, 1001440, 1001562, 1002212, 1002361, 1002604, 1004082, 1004288, 1004403]\n"
     ]
    }
   ],
   "source": [
    "print(not_matching_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 208, 10, 50)\n",
      "(174, 208, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "_id = 1001312\n",
    "folder = join(RAW_DATA, str(_id))\n",
    "_file = os.path.join(folder,'sa.nii.gz')\n",
    "nii = nib.load(_file)\n",
    "im = nii.get_fdata()\n",
    "print(im.shape)\n",
    "\n",
    "nii_es = nib.load(join(folder,'sa_ES.nii.gz'))\n",
    "im_es = nii_es.get_fdata()\n",
    "print(im_es.shape)\n",
    "print(im_es.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test the effect of nii.affine\n",
    "test_slice = im[:,:,:,10]\n",
    "test_slice_affine = nib.Nifti1Image(im[:, :, :, 10], nii.affine)\n",
    "overlap_ratio = (test_slice==test_slice).sum()/test_slice.size\n",
    "print(overlap_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "0.9908819628647215\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "best_overlap_es = 0\n",
    "es_slice = im_es[:,:,im_es.shape[2]//2]\n",
    "for i in range(50):\n",
    "    im_slice = im[:,:,im.shape[2]//2,i]\n",
    "    overlap_es = (es_slice==im_slice).sum()\n",
    "    if overlap_es > best_overlap_es:\n",
    "        best_overlap_es = overlap_es\n",
    "        best_i_es = i\n",
    "print(best_i_es)\n",
    "# best_i_es = 19\n",
    "im_slice = im[:,:,im.shape[2]//2,best_i_es]\n",
    "overlap_ratio_es = (im_slice==es_slice).sum()/es_slice.size\n",
    "print(overlap_ratio_es)\n",
    "print(np.allclose(im_slice, es_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(13,5))\n",
    "axarr[0].imshow(im_slice, cmap='gray')\n",
    "axarr[1].imshow(es_slice, cmap='gray')\n",
    "axarr[0].set_title('ES slice from sa.nii.gz')\n",
    "axarr[1].set_title('ES slice from sa_ES.nii.gz')\n",
    "plt.suptitle(f'Subject {_id}, overlap ratio: {overlap_ratio_es*100:.2f}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(join(FEATURES, f'cardiac_feature_18545_vector_labeled_noOH_dropNI_imputed.csv'))\n",
    "all_data_df = pd.read_csv(join(FEATURES, f'cardiac_feature_18545_vector_labeled_noOH_dropNI.csv'))\n",
    "field_lengths_tabular = torch.load(join(FEATURES, 'tabular_lengths.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_ids = []\n",
    "continuous_ids = []\n",
    "for i in range(len(field_lengths_tabular)):\n",
    "    if field_lengths_tabular[i] == 1:\n",
    "        continuous_ids.append(i)\n",
    "    else:\n",
    "        categorical_ids.append(i)\n",
    "continuous_ids = [x+1 for x in continuous_ids]\n",
    "categorical_ids = [x+1 for x in categorical_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]\n"
     ]
    }
   ],
   "source": [
    "print(categorical_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _id in categorical_ids:\n",
    "    print(data_df.iloc[:,_id].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfsuper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
